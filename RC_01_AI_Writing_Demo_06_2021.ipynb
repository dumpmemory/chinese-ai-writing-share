{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "RC-01: AI Writing Demo 06/2021",
      "provenance": [],
      "collapsed_sections": [
        "ipB2xHkn-asg",
        "4R1I78Xu--Jx"
      ],
      "mount_file_id": "https://github.com/hululuzhu/chinese-ai-writing-share/blob/main/RC_01_AI_Writing_Demo_06_2021.ipynb",
      "authorship_tag": "ABX9TyNPNKdz3wiR1cbbYk4s705r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/hululuzhu/chinese-ai-writing-share/blob/main/RC_01_AI_Writing_Demo_06_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Google Drive to reference [models and vocabs](https://drive.google.com/drive/folders/1d5vk9nrse4lJ55wb5zsW2wgodkwWb-V2?usp=sharing) and Initialize\r\n",
        "\r\n",
        "- Run all code and test examples by replacing chars\r\n",
        "- Please note I set topk=1 and tempature=1.0 for reproduce, play with different inference params when you run it."
      ],
      "metadata": {
        "id": "ipB2xHkn-asg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "4R1I78Xu--Jx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pickle\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "!pip install keras-transformer &> /dev/null\r\n",
        "os.environ['TF_KERAS'] = '1'\r\n",
        "from keras_transformer import get_model, decode, get_custom_objects\r\n",
        "import tensorflow as tf"
      ],
      "outputs": [],
      "metadata": {
        "id": "hCg7PwbO-4n5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Configs"
      ],
      "metadata": {
        "id": "H6SM55DRH-H-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# path varies depends on your own dir\r\n",
        "MODEL_DIR = 'drive/MyDrive/ML/Models/szhu_public_062021/'"
      ],
      "outputs": [],
      "metadata": {
        "id": "dujcdkTu_C6h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 如出错，请拷贝最开始介绍的那个Google Drive的所有文件，并mount到colab\r\n",
        "!ls {MODEL_DIR}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "couplet_model_config.pickle  couplet_vocab.pickle      poem_model.h5\n",
            "couplet_model.h5\t     poem_model_config.pickle  poem_vocab.pickle\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq-x1MtE-UUY",
        "outputId": "6284c644-d53b-444c-94f9-8922df21ec83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with open(os.path.join(MODEL_DIR, 'couplet_model_config.pickle'), 'rb') as handle:\r\n",
        "  couplet_model_config = pickle.load(handle)\r\n",
        "with open(os.path.join(MODEL_DIR, 'couplet_vocab.pickle'), 'rb') as handle:\r\n",
        "  couplet_vocab_dict = pickle.load(handle)\r\n",
        "with open(os.path.join(MODEL_DIR, 'poem_model_config.pickle'), 'rb') as handle:\r\n",
        "  poem_model_config = pickle.load(handle)\r\n",
        "with open(os.path.join(MODEL_DIR, 'poem_vocab.pickle'), 'rb') as handle:\r\n",
        "  poem_vocab_dict = pickle.load(handle)"
      ],
      "outputs": [],
      "metadata": {
        "id": "uze7aFdx_Kji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "rev_couplet_vocab_dict = {v: k for k, v in couplet_vocab_dict.items()}\r\n",
        "rev_poem_vocab_dict = {v: k for k, v in poem_vocab_dict.items()}"
      ],
      "outputs": [],
      "metadata": {
        "id": "-dkvK17OEX78"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "assert 9133 == len(couplet_vocab_dict)\r\n",
        "assert 11289 == len(poem_vocab_dict)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dMgyN03D_njJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize models and sup methods"
      ],
      "metadata": {
        "id": "CPNN8DGa_3Ro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "couplet_model = get_model(\r\n",
        "    embed_weights=np.random.random((len(couplet_vocab_dict),\r\n",
        "                                    couplet_model_config['embed_dim'])),\r\n",
        "    **couplet_model_config)\r\n",
        "couplet_model.load_weights(os.path.join(MODEL_DIR, 'couplet_model.h5'))\r\n",
        "\r\n",
        "\r\n",
        "poem_model = get_model(\r\n",
        "    embed_weights=np.random.random((len(poem_vocab_dict),\r\n",
        "                                    poem_model_config['embed_dim'])),\r\n",
        "    **poem_model_config)\r\n",
        "poem_model.load_weights(os.path.join(MODEL_DIR, 'poem_model.h5'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "-3EqiHb9_42B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "START_TOKEN_ID = poem_vocab_dict['<START>']\r\n",
        "END_TOKEN_ID = poem_vocab_dict['<END>']\r\n",
        "PAD_TOKEN_ID = poem_vocab_dict['<PAD>']\r\n",
        "\r\n",
        "COUPLET_MAX_SEQ_LEN = 34\r\n",
        "POEM_MAX_INPUT_SEQ = 14\r\n",
        "POEM_MAX_OUTPUT_SEQ = 66\r\n",
        "\r\n",
        "def couplet_inference(pre_couplet, top_k=1, temperature=1.0):\r\n",
        "  out = \"上: \" + pre_couplet + \"\\n\"\r\n",
        "  in_vector = [START_TOKEN_ID]\r\n",
        "  for c in pre_couplet:\r\n",
        "    in_vector.append(couplet_vocab_dict[c])\r\n",
        "  in_vector.append(END_TOKEN_ID)\r\n",
        "  decoded = decode(\r\n",
        "      couplet_model,\r\n",
        "      [in_vector],\r\n",
        "      start_token=couplet_vocab_dict['<START>'],\r\n",
        "      end_token=couplet_vocab_dict['<END>'],\r\n",
        "      pad_token=couplet_vocab_dict['<PAD>'],\r\n",
        "      max_len=COUPLET_MAX_SEQ_LEN,\r\n",
        "      top_k=top_k,\r\n",
        "      temperature=temperature,\r\n",
        "  )\r\n",
        "  for i in range(len(decoded)):\r\n",
        "    out += '下: ' + ''.join(map(lambda x: rev_couplet_vocab_dict[x],\r\n",
        "                       decoded[i][1:-1]))\r\n",
        "  print(out)\r\n",
        "\r\n",
        "def poem_encode(raw_text, is_decode_input, is_decode_output):\r\n",
        "  assert not (is_decode_input and is_decode_output)\r\n",
        "  output = []\r\n",
        "  if not is_decode_output:\r\n",
        "    output.append(START_TOKEN_ID)\r\n",
        "  for c in raw_text:\r\n",
        "    output.append(poem_vocab_dict[c])\r\n",
        "  output.append(END_TOKEN_ID)\r\n",
        "  # padding\r\n",
        "  total_size = POEM_MAX_OUTPUT_SEQ if is_decode_input or is_decode_output else POEM_MAX_INPUT_SEQ\r\n",
        "  for i in range(total_size - len(output)):\r\n",
        "    output.append(PAD_TOKEN_ID)\r\n",
        "  return output\r\n",
        "\r\n",
        "def poem_decode(token_ids):\r\n",
        "  output = \"\"\r\n",
        "  for token_id in token_ids:\r\n",
        "    if token_id > 2:\r\n",
        "      output += rev_poem_vocab_dict[token_id]\r\n",
        "    elif token_id == 0:\r\n",
        "      break\r\n",
        "  return output\r\n",
        "\r\n",
        "def poem_inference(title, top_k=1, temperature=1.0):\r\n",
        "  out = \"标题: \" + title + \"\\n\"\r\n",
        "  decoded = decode(\r\n",
        "      poem_model,\r\n",
        "      poem_encode(title, False, False),\r\n",
        "      start_token=START_TOKEN_ID,\r\n",
        "      end_token=END_TOKEN_ID,\r\n",
        "      pad_token=PAD_TOKEN_ID,\r\n",
        "      max_len=POEM_MAX_OUTPUT_SEQ,\r\n",
        "      top_k=top_k,\r\n",
        "      temperature=temperature,\r\n",
        "  )\r\n",
        "  out += \"正文: \" + poem_decode(decoded)\r\n",
        "  print(out)\r\n",
        "\r\n",
        "poem_inference('秋思')\r\n",
        "couplet_inference('欢天喜地度佳节')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "标题: 秋思\n",
            "正文: 秋风吹雨过，秋色满江城。一叶无人到，千山有客情。\n",
            "上: 欢天喜地度佳节\n",
            "下: 举国迎春贺新年\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hrRPle-D89G",
        "outputId": "9eb81da6-1208-4ef9-ed09-364abab07801"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "zIrhA85CHWcI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for pre in ['欢天喜地度佳节', '不待鸣钟已汗颜，重来试手竟何艰',\r\n",
        "            '当年欲跃龙门去，今日真披马革还', '载歌在谷']:\r\n",
        "  couplet_inference(pre, top_k=1, temperature=1.0)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "上: 欢天喜地度佳节\n",
            "下: 举国迎春贺新年\n",
            "上: 不待鸣钟已汗颜，重来试手竟何艰\n",
            "下: 只缘沧海常风雨，再去翻身只等闲\n",
            "上: 当年欲跃龙门去，今日真披马革还\n",
            "下: 此际重逢凤阙来，明朝再赋凤凰鸣\n",
            "上: 载歌在谷\n",
            "下: 如醉如痴\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKkbnDPHGbGo",
        "outputId": "23fa293c-84e7-44fe-fe2e-08e1039324f1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for t in ['秋思', '百度', '湾区春日之谜', '自由而无用之灵魂']:\n",
        "  poem_inference(t, top_k=1, temperature=1.0)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "标题: 秋思\n",
            "正文: 秋风吹雨过，秋色满江城。一叶无人到，千山有客情。\n",
            "标题: 百度\n",
            "正文: 百尺孤城上，千金万里中。山川无限水，水石有余风。\n",
            "标题: 湾区春日之谜\n",
            "正文: 春风吹雨不成秋，春色如何一日休。不是春光无处着，只应春色是人愁。\n",
            "标题: 自由而无用之灵魂\n",
            "正文: 我生不知，不识不知。我之不知，我之不知。我亦不知，不如不知。我亦不知，不知何爲。\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx-xnGr_E--u",
        "outputId": "e6040df9-efbc-4aea-e2e3-38aa10964edf"
      }
    }
  ]
}