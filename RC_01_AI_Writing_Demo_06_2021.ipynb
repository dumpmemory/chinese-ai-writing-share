{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RC-01: AI Writing Demo 06/2021",
      "provenance": [],
      "collapsed_sections": [
        "ipB2xHkn-asg",
        "4R1I78Xu--Jx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipB2xHkn-asg"
      },
      "source": [
        "# Connect to Google Drive to reference [models and vocabs](https://drive.google.com/drive/folders/1d5vk9nrse4lJ55wb5zsW2wgodkwWb-V2?usp=sharing) and Initialize\n",
        "\n",
        "- Run all code and test examples by replacing chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R1I78Xu--Jx"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCg7PwbO-4n5"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install keras-transformer &> /dev/null\n",
        "os.environ['TF_KERAS'] = '1'\n",
        "from keras_transformer import get_model, decode, get_custom_objects\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6SM55DRH-H-"
      },
      "source": [
        "## Load Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dujcdkTu_C6h"
      },
      "source": [
        "# path varies depends on your own dir\n",
        "MODEL_DIR = 'drive/MyDrive/ML/Models/szhu_public_062021/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq-x1MtE-UUY",
        "outputId": "5598bcca-ecae-431a-e97c-3f7e9f6c535a"
      },
      "source": [
        "!ls {MODEL_DIR}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "couplet_model_config.pickle  couplet_vocab.pickle      poem_model.h5\n",
            "couplet_model.h5\t     poem_model_config.pickle  poem_vocab.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uze7aFdx_Kji"
      },
      "source": [
        "with open(os.path.join(MODEL_DIR, 'couplet_model_config.pickle'), 'rb') as handle:\n",
        "  couplet_model_config = pickle.load(handle)\n",
        "with open(os.path.join(MODEL_DIR, 'couplet_vocab.pickle'), 'rb') as handle:\n",
        "  couplet_vocab_dict = pickle.load(handle)\n",
        "with open(os.path.join(MODEL_DIR, 'poem_model_config.pickle'), 'rb') as handle:\n",
        "  poem_model_config = pickle.load(handle)\n",
        "with open(os.path.join(MODEL_DIR, 'poem_vocab.pickle'), 'rb') as handle:\n",
        "  poem_vocab_dict = pickle.load(handle)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dkvK17OEX78"
      },
      "source": [
        "rev_couplet_vocab_dict = {v: k for k, v in couplet_vocab_dict.items()}\n",
        "rev_poem_vocab_dict = {v: k for k, v in poem_vocab_dict.items()}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMgyN03D_njJ"
      },
      "source": [
        "assert 9133 == len(couplet_vocab_dict)\n",
        "assert 11289 == len(poem_vocab_dict)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPNN8DGa_3Ro"
      },
      "source": [
        "## Initialize models and sup methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3EqiHb9_42B"
      },
      "source": [
        "couplet_model = get_model(\n",
        "    embed_weights=np.random.random((len(couplet_vocab_dict),\n",
        "                                    couplet_model_config['embed_dim'])),\n",
        "    **couplet_model_config)\n",
        "couplet_model.load_weights(os.path.join(MODEL_DIR, 'couplet_model.h5'))\n",
        "\n",
        "\n",
        "poem_model = get_model(\n",
        "    embed_weights=np.random.random((len(poem_vocab_dict),\n",
        "                                    poem_model_config['embed_dim'])),\n",
        "    **poem_model_config)\n",
        "poem_model.load_weights(os.path.join(MODEL_DIR, 'poem_model.h5'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hrRPle-D89G",
        "outputId": "025ee235-e9a6-4cd8-c546-2892284c9720"
      },
      "source": [
        "START_TOKEN_ID = poem_vocab_dict['<START>']\n",
        "END_TOKEN_ID = poem_vocab_dict['<END>']\n",
        "PAD_TOKEN_ID = poem_vocab_dict['<PAD>']\n",
        "\n",
        "COUPLET_MAX_SEQ_LEN = 34\n",
        "POEM_MAX_INPUT_SEQ = 14\n",
        "POEM_MAX_OUTPUT_SEQ = 66\n",
        "\n",
        "def couplet_inference(pre_couplet):\n",
        "  out = \"上: \" + pre_couplet + \"\\n\"\n",
        "  in_vector = [START_TOKEN_ID]\n",
        "  for c in pre_couplet:\n",
        "    in_vector.append(couplet_vocab_dict[c])\n",
        "  in_vector.append(END_TOKEN_ID)\n",
        "  decoded = decode(\n",
        "      couplet_model,\n",
        "      [in_vector],\n",
        "      start_token=couplet_vocab_dict['<START>'],\n",
        "      end_token=couplet_vocab_dict['<END>'],\n",
        "      pad_token=couplet_vocab_dict['<PAD>'],\n",
        "      max_len=COUPLET_MAX_SEQ_LEN,\n",
        "      top_k=1,\n",
        "      temperature=1,\n",
        "  )\n",
        "  for i in range(len(decoded)):\n",
        "    out += '下: ' + ''.join(map(lambda x: rev_couplet_vocab_dict[x],\n",
        "                       decoded[i][1:-1]))\n",
        "  print(out)\n",
        "\n",
        "def poem_encode(raw_text, is_decode_input, is_decode_output):\n",
        "  assert not (is_decode_input and is_decode_output)\n",
        "  output = []\n",
        "  if not is_decode_output:\n",
        "    output.append(START_TOKEN_ID)\n",
        "  for c in raw_text:\n",
        "    output.append(poem_vocab_dict[c])\n",
        "  output.append(END_TOKEN_ID)\n",
        "  # padding\n",
        "  total_size = POEM_MAX_OUTPUT_SEQ if is_decode_input or is_decode_output else POEM_MAX_INPUT_SEQ\n",
        "  for i in range(total_size - len(output)):\n",
        "    output.append(PAD_TOKEN_ID)\n",
        "  return output\n",
        "\n",
        "def poem_decode(token_ids):\n",
        "  output = \"\"\n",
        "  for token_id in token_ids:\n",
        "    if token_id > 2:\n",
        "      output += rev_poem_vocab_dict[token_id]\n",
        "    elif token_id == 0:\n",
        "      break\n",
        "  return output\n",
        "\n",
        "def poem_inference(title):\n",
        "  out = \"标题: \" + title + \"\\n\"\n",
        "  decoded = decode(\n",
        "      poem_model,\n",
        "      poem_encode(title, False, False),\n",
        "      start_token=START_TOKEN_ID,\n",
        "      end_token=END_TOKEN_ID,\n",
        "      pad_token=PAD_TOKEN_ID,\n",
        "      max_len=POEM_MAX_OUTPUT_SEQ,\n",
        "      top_k=1,\n",
        "      temperature=1.0,\n",
        "  )\n",
        "  out += \"正文: \" + poem_decode(decoded)\n",
        "  print(out)\n",
        "\n",
        "poem_inference('秋思')\n",
        "couplet_inference('欢天喜地度佳节')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "标题: 秋思\n",
            "正文: 秋风吹雨过，秋色满江城。一叶无人到，千山有客情。\n",
            "上: 欢天喜地度佳节\n",
            "下: 举国迎春贺新年\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIrhA85CHWcI"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx-xnGr_E--u",
        "outputId": "5e4b40d7-78c9-4533-e61c-0b5e3331bae6"
      },
      "source": [
        "for t in ['秋思', '百度', '湾区春日之谜']:\n",
        "  poem_inference(t)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "标题: 秋思\n",
            "正文: 秋风吹雨过，秋色满江城。一叶无人到，千山有客情。\n",
            "标题: 百度\n",
            "正文: 百尺孤城上，千金万里中。山川无限水，水石有余风。\n",
            "标题: 湾区春日之谜\n",
            "正文: 春风吹雨不成秋，春色如何一日休。不是春光无处着，只应春色是人愁。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKkbnDPHGbGo",
        "outputId": "e51b1b89-3552-4af1-e119-42ee6de4d8be"
      },
      "source": [
        "for pre in ['欢天喜地度佳节', '不待鸣钟已汗颜，重来试手竟何艰', '当年欲跃龙门去，今日真披马革还']:\n",
        "  couplet_inference(pre)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "上: 欢天喜地度佳节\n",
            "下: 举国迎春贺新年\n",
            "上: 不待鸣钟已汗颜，重来试手竟何艰\n",
            "下: 只缘沧海常风雨，再去翻身只等闲\n",
            "上: 当年欲跃龙门去，今日真披马革还\n",
            "下: 此际重逢凤阙来，明朝再赋凤凰鸣\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}