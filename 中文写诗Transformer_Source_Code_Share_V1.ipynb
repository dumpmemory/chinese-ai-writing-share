{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "中文写诗Transformer Source Code Share V1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hululuzhu/chinese-ai-writing-share/blob/main/%E4%B8%AD%E6%96%87%E5%86%99%E8%AF%97Transformer_Source_Code_Share_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opLDHSMPBx5w"
      },
      "source": [
        "# Colab to train a Chinese poem writing transformer. e.g.\n",
        "\n",
        "```\n",
        "标题: 秋思\n",
        "正文: 秋风吹雨过，秋色满江城。一叶无人到，千山有客情。\n",
        "标题: 百度\n",
        "正文: 百尺孤城上，千金万里中。山川无限水，水石有余风。\n",
        "标题: 湾区春日之谜\n",
        "正文: 春风吹雨不成秋，春色如何一日休。不是春光无处着，只应春色是人愁。\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlJ3e7rtOSGp"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqtm9Zw_OTby"
      },
      "source": [
        "import json\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "!pip install -q \"tqdm>=4.36.1\" > /tmp/na\n",
        "from tqdm.notebook import tqdm\n",
        "!pip install chinese-converter > /tmp/na\n",
        "import chinese_converter\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install keras-transformer &> /dev/null\n",
        "os.environ['TF_KERAS'] = '1'\n",
        "from keras_transformer import get_model, decode, get_custom_objects\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-VN2c0Sgpbb"
      },
      "source": [
        "## TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmqV6lGhgpbc"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av1LR9NcssbX"
      },
      "source": [
        "# Connect to Google Drive for storage\n",
        "- useful to store model or dict/params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDHpUVGasvE7"
      },
      "source": [
        "!mkdir -p drive/MyDrive/ML/Models/chinese_poem_v1\n",
        "WORK_DIR = 'drive/MyDrive/ML/Models/chinese_poem_v1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mmgXuGWs5M2"
      },
      "source": [
        "# Load data and transform and persist to Drive (no need to rerun)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai47AGm0kQ7t"
      },
      "source": [
        "# https://github.com/chinese-poetry/chinese-poetry\n",
        "POEM_CONTENT = {\n",
        "    'tang': {\n",
        "        'total': 58,\n",
        "        'pattern': \"https://raw.githubusercontent.com/chinese-poetry/chinese-poetry/master/json/poet.tang.{0}.json\"\n",
        "    },\n",
        "    'song': {\n",
        "        'total': 255,\n",
        "        'pattern': \"https://raw.githubusercontent.com/chinese-poetry/chinese-poetry/master/json/poet.song.{0}.json\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def get_poems(is_test=True, verbose=True):\n",
        "  df_list = []\n",
        "  for dynasty in POEM_CONTENT:\n",
        "    size = 3 if is_test else POEM_CONTENT[dynasty]['total']\n",
        "    pbar = tqdm(total=size, desc=\"Dynasty \" + dynasty)\n",
        "    for i in range(size):\n",
        "      url = POEM_CONTENT[dynasty]['pattern'].format(i * 1000)\n",
        "      if verbose:\n",
        "        print(f\"download {url} now\")\n",
        "      df_list.append(pd.read_json(url))\n",
        "      pbar.update(1)\n",
        "  return pd.concat(df_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2neo-AtkTfB"
      },
      "source": [
        "df = get_poems(is_test=False, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxa5z5QVkV_g"
      },
      "source": [
        "df['concat_paragraphs'] = [''.join(map(str, l)) for l in df['paragraphs']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuDXzadwnMND"
      },
      "source": [
        "df = df[['author', 'title', 'concat_paragraphs']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH4suWB3oZFA"
      },
      "source": [
        "## Convert to simplified Chinese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-T17tTioi_Q"
      },
      "source": [
        "def convert_schinese(tchinese):\n",
        "  return chinese_converter.to_simplified(tchinese)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pD5v3ILoobP"
      },
      "source": [
        "df['s_content'] = df.apply(lambda row: convert_schinese(''.join(row.concat_paragraphs)), axis=1)\n",
        "df['s_title'] = df.apply(lambda row: convert_schinese(''.join(row.title)), axis=1)\n",
        "df['s_author'] = df.apply(lambda row: convert_schinese(''.join(row.author)), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6NP9Qcu0OjA"
      },
      "source": [
        "my_df = df[['s_content', 's_title', 's_author']]\n",
        "my_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrQ7NtkXyOTZ"
      },
      "source": [
        "for key in my_df.columns:\n",
        "  print(my_df[key][:].apply(len).describe())\n",
        "\n",
        "def trim_author_fn(row):\n",
        "  return row.s_author[:4]\n",
        "\n",
        "def trim_title_fn(row):\n",
        "  trimed_title = row.s_title[:12].replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "  return trimed_title\n",
        "\n",
        "def trim_content_fn(row):\n",
        "  trimed_content = row.s_content[:64]\n",
        "  last_period = trimed_content.rfind(\"。\")\n",
        "  return trimed_content[:last_period+1]\n",
        "\n",
        "# Trim the size\n",
        "my_df['s_author'] = my_df.apply(trim_author_fn, axis=1)\n",
        "my_df['s_title'] = my_df.apply(trim_title_fn, axis=1)\n",
        "my_df['s_content'] = my_df.apply(trim_content_fn, axis=1)\n",
        "\n",
        "\n",
        "# TODO, find space in title and choose 1st part\n",
        "# TODO, find last period of content and stop there after triming"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K02qD15vvhsj"
      },
      "source": [
        "short_mask = (my_df['s_title'].str.len() == 0) | (my_df['s_content'].str.len() <= 10) | ('无正文' == my_df['s_content']) | ('无正文' == my_df['s_author'])\n",
        "filter_my_df = my_df.loc[~short_mask]\n",
        "filter_my_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5mBqixf7Oie"
      },
      "source": [
        "filter_my_df[filter_my_df['s_content'].str.len() <= 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVvEZpiopbqQ"
      },
      "source": [
        "## Get Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A847PRQUpcyc"
      },
      "source": [
        "token_dict = {\n",
        "  '<PAD>': 0,\n",
        "  '<START>': 1,\n",
        "  '<END>': 2,\n",
        "}\n",
        "\n",
        "def process_token(token_dict, df):\n",
        "  for field in df.columns:\n",
        "    for title in df[field]:\n",
        "      for c in title:\n",
        "        if c not in token_dict:\n",
        "          token_dict[c] = len(token_dict)\n",
        "\n",
        "process_token(token_dict, filter_my_df)\n",
        "rev_token_dict = {v: k for k, v in token_dict.items()}\n",
        "vocab_size = len(token_dict)\n",
        "\n",
        "print(\"vocab_size\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y6Vdq3C-piK"
      },
      "source": [
        "## Persist DF and Dictionary for future use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yLcSBBn-s0D"
      },
      "source": [
        "with open(os.path.join(WORK_DIR, 'vocab_0604_v1.pickle'), 'wb') as handle:\n",
        "    pickle.dump(token_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "filter_my_df.to_pickle(os.path.join(WORK_DIR, 'dataframe_300k_0604_v1.pickle'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8lCv9FA_Wo9"
      },
      "source": [
        "!ls -l {WORK_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuL7sHzvANuD"
      },
      "source": [
        "# Train model from Title to Content (without author)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzP1TBto_jUB"
      },
      "source": [
        "## Reload from storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUZYJjAM_rQD"
      },
      "source": [
        "loaded_token_dict = pickle.load(\n",
        "    open(os.path.join(WORK_DIR, 'vocab_0604_v1.pickle'), \"rb\" ))\n",
        "\n",
        "loaded_df = pd.read_pickle(\n",
        "    os.path.join(WORK_DIR, 'dataframe_300k_0604_v1.pickle'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h9Km6zu_l5q"
      },
      "source": [
        "rev_token_dict = {v: k for k, v in loaded_token_dict.items()}\n",
        "\n",
        "assert 11289 == len(rev_token_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQS0KSuNO7dC"
      },
      "source": [
        "## Encode data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAW3g-ZNEn9a"
      },
      "source": [
        "MAX_INPUT_SEQ = 14 # max title length + 2 special tokens\n",
        "MAX_OUTPUT_SEQ = 66 # max 64 content length + 2 special tokens\n",
        "START_TOKEN_ID = loaded_token_dict['<START>']\n",
        "END_TOKEN_ID = loaded_token_dict['<END>']\n",
        "PAD_TOKEN_ID = loaded_token_dict['<PAD>']\n",
        "\n",
        "\n",
        "def encode(raw_text, is_decode_input, is_decode_output):\n",
        "  assert not (is_decode_input and is_decode_output)\n",
        "  output = []\n",
        "  if not is_decode_output:\n",
        "    output.append(START_TOKEN_ID)\n",
        "  for c in raw_text:\n",
        "    output.append(loaded_token_dict[c])\n",
        "  output.append(END_TOKEN_ID)\n",
        "  # padding\n",
        "  total_size = MAX_OUTPUT_SEQ if is_decode_input or is_decode_output else MAX_INPUT_SEQ\n",
        "  for i in range(total_size - len(output)):\n",
        "    output.append(PAD_TOKEN_ID)\n",
        "  return output\n",
        "\n",
        "def decode(token_ids):\n",
        "  output = \"\"\n",
        "  for token_id in token_ids:\n",
        "    if token_id > 2:\n",
        "      output += rev_token_dict[token_id]\n",
        "    elif token_id == 0:\n",
        "      break\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z07rA2VF0-k"
      },
      "source": [
        "print(encode('登竺云山', is_decode_input = False, is_decode_output = False))\n",
        "print(encode('独上千峰与万峰，晴岚淡写海江容', is_decode_input = True, is_decode_output = False))\n",
        "print(encode('独上千峰与万峰，晴岚淡写海江容', is_decode_input = False, is_decode_output = True))\n",
        "\n",
        "print(decode([1, 546, 4787, 35, 344, 2, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
        "print(decode([1, 302, 167, 17, 168, 481, 185, 168, 8, 773, 2281, 3939, 94, 342, 1566, 1563, 2, 0, 0,]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRXAws88JwoV"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# Shuffle the order of df\n",
        "# TEST_RATE = 0.03\n",
        "shuffle_loaded_df = loaded_df.sample(frac=1).reset_index(drop=True)\n",
        "cutoff = 6000 # Use 6k as test\n",
        "df_test = shuffle_loaded_df[:cutoff]\n",
        "df_train = shuffle_loaded_df[cutoff:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSGMIKe7KJk_"
      },
      "source": [
        "def prepare_ds(df):\n",
        "  text_x = df['s_title'].values\n",
        "  text_y = df['s_content'].values\n",
        "  x = np.asarray([encode(k, False, False) for k in text_x])\n",
        "  x_d = np.asarray([encode(k, True, False) for k in text_y])\n",
        "  # final output need extra 1 dim\n",
        "  y = np.expand_dims(np.asarray([encode(k, False, True) for k in text_y]), -1)\n",
        "  return x, x_d, y\n",
        "\n",
        "train_x, train_x_d, train_y = prepare_ds(df_train)\n",
        "test_x, test_x_d, test_y = prepare_ds(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ylQu67xM0U6"
      },
      "source": [
        "print(train_x.shape, train_x_d.shape, train_y.shape)\n",
        "print(test_x.shape, test_x_d.shape, test_y.shape)\n",
        "\n",
        "print(decode(train_x[1000]), decode(train_x_d[1000]), decode(np.squeeze(train_y[1000], -1)))\n",
        "print(decode(test_x[1000]), decode(test_x_d[1000]), decode(np.squeeze(test_y[1000], -1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALWUKqUsPofT"
      },
      "source": [
        "## Build transformer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVne6yNePqjc"
      },
      "source": [
        "with strategy.scope():\n",
        "  num_encoders = 4\n",
        "  num_docoders = 4\n",
        "  num_heads = 8\n",
        "  embed_size = 64 * num_docoders\n",
        "  drop_out_rate = 0.3\n",
        "  model = get_model(\n",
        "    token_num=len(rev_token_dict),\n",
        "    embed_dim=embed_size,\n",
        "    encoder_num=num_encoders,\n",
        "    decoder_num=num_docoders,\n",
        "    head_num=num_heads,\n",
        "    hidden_dim=embed_size,\n",
        "    attention_activation='gelu',\n",
        "    feed_forward_activation='gelu',\n",
        "    dropout_rate=drop_out_rate,\n",
        "    embed_weights=np.random.random((len(rev_token_dict), embed_size)),\n",
        "  )\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE3KmHYUQX5G"
      },
      "source": [
        "epochs = 60  # 60 is minimal to be meaningful\n",
        "batch_size = 128\n",
        "model.fit(\n",
        "  x=[train_x, train_x_d],\n",
        "  y=train_y,\n",
        "  batch_size=batch_size,\n",
        "  epochs=epochs,\n",
        "  validation_data=([test_x, test_x_d], test_y),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP3Br5JAgpbm"
      },
      "source": [
        "# Save your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9Lw-zuxgpbn"
      },
      "source": [
        "!mkdir -p {WORK_DIR}/model_weights\n",
        "model.save_weights(f'{WORK_DIR}/model_weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6bU4cYq6e66"
      },
      "source": [
        "# Inference, please see [this colab](https://github.com/hululuzhu/chinese-ai-writing-share/blob/main/RC_01_AI_Writing_Demo_06_2021.ipynb)"
      ]
    }
  ]
}