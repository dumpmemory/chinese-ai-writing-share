{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "中文写诗Transformer Source Code Share V1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1g6PtycXS6ukrbp9WI9cISeqksY-iUL2x",
      "authorship_tag": "ABX9TyMup8+SFl3jIRvMu+c3WjsB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/hululuzhu/chinese-ai-writing-share/blob/main/%E4%B8%AD%E6%96%87%E5%86%99%E8%AF%97Transformer_Source_Code_Share_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab to train a Chinese poem writing transformer. e.g.\n",
        "\n",
        "```\n",
        "标题: 秋思\n",
        "正文: 秋风吹雨过，秋色满江城。一叶无人到，千山有客情。\n",
        "标题: 百度\n",
        "正文: 百尺孤城上，千金万里中。山川无限水，水石有余风。\n",
        "标题: 湾区春日之谜\n",
        "正文: 春风吹雨不成秋，春色如何一日休。不是春光无处着，只应春色是人愁。\n",
        "```"
      ],
      "metadata": {
        "id": "opLDHSMPBx5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "PlJ3e7rtOSGp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\r\n",
        "import urllib.request\r\n",
        "import pandas as pd\r\n",
        "!pip install -q \"tqdm>=4.36.1\" > /tmp/na\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "!pip install chinese-converter > /tmp/na\r\n",
        "import chinese_converter\r\n",
        "import pickle\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "!pip install keras-transformer &> /dev/null\r\n",
        "os.environ['TF_KERAS'] = '1'\r\n",
        "from keras_transformer import get_model, decode, get_custom_objects\r\n",
        "import tensorflow as tf"
      ],
      "outputs": [],
      "metadata": {
        "id": "Xqtm9Zw_OTby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TPU"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
        "tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
        "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\r\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Google Drive for storage\n",
        "- useful to store model or dict/params"
      ],
      "metadata": {
        "id": "av1LR9NcssbX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!mkdir -p drive/MyDrive/ML/Models/chinese_poem_v1\r\n",
        "WORK_DIR = 'drive/MyDrive/ML/Models/chinese_poem_v1'"
      ],
      "outputs": [],
      "metadata": {
        "id": "iDHpUVGasvE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data and transform and persist to Drive (no need to rerun)"
      ],
      "metadata": {
        "id": "0mmgXuGWs5M2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# https://github.com/chinese-poetry/chinese-poetry\r\n",
        "POEM_CONTENT = {\r\n",
        "    'tang': {\r\n",
        "        'total': 58,\r\n",
        "        'pattern': \"https://raw.githubusercontent.com/chinese-poetry/chinese-poetry/master/json/poet.tang.{0}.json\"\r\n",
        "    },\r\n",
        "    'song': {\r\n",
        "        'total': 255,\r\n",
        "        'pattern': \"https://raw.githubusercontent.com/chinese-poetry/chinese-poetry/master/json/poet.song.{0}.json\"\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "def get_poems(is_test=True, verbose=True):\r\n",
        "  df_list = []\r\n",
        "  for dynasty in POEM_CONTENT:\r\n",
        "    size = 3 if is_test else POEM_CONTENT[dynasty]['total']\r\n",
        "    pbar = tqdm(total=size, desc=\"Dynasty \" + dynasty)\r\n",
        "    for i in range(size):\r\n",
        "      url = POEM_CONTENT[dynasty]['pattern'].format(i * 1000)\r\n",
        "      if verbose:\r\n",
        "        print(f\"download {url} now\")\r\n",
        "      df_list.append(pd.read_json(url))\r\n",
        "      pbar.update(1)\r\n",
        "  return pd.concat(df_list)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ai47AGm0kQ7t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = get_poems(is_test=False, verbose=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "d2neo-AtkTfB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df['concat_paragraphs'] = [''.join(map(str, l)) for l in df['paragraphs']]"
      ],
      "outputs": [],
      "metadata": {
        "id": "Uxa5z5QVkV_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = df[['author', 'title', 'concat_paragraphs']]"
      ],
      "outputs": [],
      "metadata": {
        "id": "CuDXzadwnMND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to simplified Chinese"
      ],
      "metadata": {
        "id": "LH4suWB3oZFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def convert_schinese(tchinese):\r\n",
        "  return chinese_converter.to_simplified(tchinese)"
      ],
      "outputs": [],
      "metadata": {
        "id": "r-T17tTioi_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df['s_content'] = df.apply(lambda row: convert_schinese(''.join(row.concat_paragraphs)), axis=1)\r\n",
        "df['s_title'] = df.apply(lambda row: convert_schinese(''.join(row.title)), axis=1)\r\n",
        "df['s_author'] = df.apply(lambda row: convert_schinese(''.join(row.author)), axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9pD5v3ILoobP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "my_df = df[['s_content', 's_title', 's_author']]\r\n",
        "my_df"
      ],
      "outputs": [],
      "metadata": {
        "id": "W6NP9Qcu0OjA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for key in my_df.columns:\r\n",
        "  print(my_df[key][:].apply(len).describe())\r\n",
        "\r\n",
        "def trim_author_fn(row):\r\n",
        "  return row.s_author[:4]\r\n",
        "\r\n",
        "def trim_title_fn(row):\r\n",
        "  trimed_title = row.s_title[:12].replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\r\n",
        "  return trimed_title\r\n",
        "\r\n",
        "def trim_content_fn(row):\r\n",
        "  trimed_content = row.s_content[:64]\r\n",
        "  last_period = trimed_content.rfind(\"。\")\r\n",
        "  return trimed_content[:last_period+1]\r\n",
        "\r\n",
        "# Trim the size\r\n",
        "my_df['s_author'] = my_df.apply(trim_author_fn, axis=1)\r\n",
        "my_df['s_title'] = my_df.apply(trim_title_fn, axis=1)\r\n",
        "my_df['s_content'] = my_df.apply(trim_content_fn, axis=1)\r\n",
        "\r\n",
        "\r\n",
        "# TODO, find space in title and choose 1st part\r\n",
        "# TODO, find last period of content and stop there after triming"
      ],
      "outputs": [],
      "metadata": {
        "id": "KrQ7NtkXyOTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "short_mask = (my_df['s_title'].str.len() == 0) | (my_df['s_content'].str.len() <= 10) | ('无正文' == my_df['s_content']) | ('无正文' == my_df['s_author'])\r\n",
        "filter_my_df = my_df.loc[~short_mask]\r\n",
        "filter_my_df"
      ],
      "outputs": [],
      "metadata": {
        "id": "K02qD15vvhsj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "filter_my_df[filter_my_df['s_content'].str.len() <= 10]"
      ],
      "outputs": [],
      "metadata": {
        "id": "P5mBqixf7Oie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Dictionary"
      ],
      "metadata": {
        "id": "PVvEZpiopbqQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "token_dict = {\r\n",
        "  '<PAD>': 0,\r\n",
        "  '<START>': 1,\r\n",
        "  '<END>': 2,\r\n",
        "}\r\n",
        "\r\n",
        "def process_token(token_dict, df):\r\n",
        "  for field in df.columns:\r\n",
        "    for title in df[field]:\r\n",
        "      for c in title:\r\n",
        "        if c not in token_dict:\r\n",
        "          token_dict[c] = len(token_dict)\r\n",
        "\r\n",
        "process_token(token_dict, filter_my_df)\r\n",
        "rev_token_dict = {v: k for k, v in token_dict.items()}\r\n",
        "vocab_size = len(token_dict)\r\n",
        "\r\n",
        "print(\"vocab_size\", vocab_size)"
      ],
      "outputs": [],
      "metadata": {
        "id": "A847PRQUpcyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Persist DF and Dictionary for future use"
      ],
      "metadata": {
        "id": "5y6Vdq3C-piK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with open(os.path.join(WORK_DIR, 'vocab_0604_v1.pickle'), 'wb') as handle:\r\n",
        "    pickle.dump(token_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "filter_my_df.to_pickle(os.path.join(WORK_DIR, 'dataframe_300k_0604_v1.pickle'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "6yLcSBBn-s0D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!ls -l {WORK_DIR}"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q8lCv9FA_Wo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model from Title to Content (without author)"
      ],
      "metadata": {
        "id": "PuL7sHzvANuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reload from storage"
      ],
      "metadata": {
        "id": "FzP1TBto_jUB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "loaded_token_dict = pickle.load(\r\n",
        "    open(os.path.join(WORK_DIR, 'vocab_0604_v1.pickle'), \"rb\" ))\r\n",
        "\r\n",
        "loaded_df = pd.read_pickle(\r\n",
        "    os.path.join(WORK_DIR, 'dataframe_300k_0604_v1.pickle'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "QUZYJjAM_rQD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "rev_token_dict = {v: k for k, v in loaded_token_dict.items()}\r\n",
        "\r\n",
        "assert 11289 == len(rev_token_dict)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9h9Km6zu_l5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode data"
      ],
      "metadata": {
        "id": "RQS0KSuNO7dC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "MAX_INPUT_SEQ = 14 # max title length + 2 special tokens\r\n",
        "MAX_OUTPUT_SEQ = 66 # max 64 content length + 2 special tokens\r\n",
        "START_TOKEN_ID = loaded_token_dict['<START>']\r\n",
        "END_TOKEN_ID = loaded_token_dict['<END>']\r\n",
        "PAD_TOKEN_ID = loaded_token_dict['<PAD>']\r\n",
        "\r\n",
        "\r\n",
        "def encode(raw_text, is_decode_input, is_decode_output):\r\n",
        "  assert not (is_decode_input and is_decode_output)\r\n",
        "  output = []\r\n",
        "  if not is_decode_output:\r\n",
        "    output.append(START_TOKEN_ID)\r\n",
        "  for c in raw_text:\r\n",
        "    output.append(loaded_token_dict[c])\r\n",
        "  output.append(END_TOKEN_ID)\r\n",
        "  # padding\r\n",
        "  total_size = MAX_OUTPUT_SEQ if is_decode_input or is_decode_output else MAX_INPUT_SEQ\r\n",
        "  for i in range(total_size - len(output)):\r\n",
        "    output.append(PAD_TOKEN_ID)\r\n",
        "  return output\r\n",
        "\r\n",
        "def decode(token_ids):\r\n",
        "  output = \"\"\r\n",
        "  for token_id in token_ids:\r\n",
        "    if token_id > 2:\r\n",
        "      output += rev_token_dict[token_id]\r\n",
        "    elif token_id == 0:\r\n",
        "      break\r\n",
        "  return output"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZAW3g-ZNEn9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(encode('登竺云山', is_decode_input = False, is_decode_output = False))\r\n",
        "print(encode('独上千峰与万峰，晴岚淡写海江容', is_decode_input = True, is_decode_output = False))\r\n",
        "print(encode('独上千峰与万峰，晴岚淡写海江容', is_decode_input = False, is_decode_output = True))\r\n",
        "\r\n",
        "print(decode([1, 546, 4787, 35, 344, 2, 0, 0, 0, 0, 0, 0, 0, 0]))\r\n",
        "print(decode([1, 302, 167, 17, 168, 481, 185, 168, 8, 773, 2281, 3939, 94, 342, 1566, 1563, 2, 0, 0,]))"
      ],
      "outputs": [],
      "metadata": {
        "id": "8z07rA2VF0-k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# from sklearn.model_selection import train_test_split\r\n",
        "# Shuffle the order of df\r\n",
        "# TEST_RATE = 0.03\r\n",
        "shuffle_loaded_df = loaded_df.sample(frac=1).reset_index(drop=True)\r\n",
        "cutoff = 6000 # Use 6k as test\r\n",
        "df_test = shuffle_loaded_df[:cutoff]\r\n",
        "df_train = shuffle_loaded_df[cutoff:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "WRXAws88JwoV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def prepare_ds(df):\r\n",
        "  text_x = df['s_title'].values\r\n",
        "  text_y = df['s_content'].values\r\n",
        "  x = np.asarray([encode(k, False, False) for k in text_x])\r\n",
        "  x_d = np.asarray([encode(k, True, False) for k in text_y])\r\n",
        "  # final output need extra 1 dim\r\n",
        "  y = np.expand_dims(np.asarray([encode(k, False, True) for k in text_y]), -1)\r\n",
        "  return x, x_d, y\r\n",
        "\r\n",
        "train_x, train_x_d, train_y = prepare_ds(df_train)\r\n",
        "test_x, test_x_d, test_y = prepare_ds(df_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gSGMIKe7KJk_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(train_x.shape, train_x_d.shape, train_y.shape)\r\n",
        "print(test_x.shape, test_x_d.shape, test_y.shape)\r\n",
        "\r\n",
        "print(decode(train_x[1000]), decode(train_x_d[1000]), decode(np.squeeze(train_y[1000], -1)))\r\n",
        "print(decode(test_x[1000]), decode(test_x_d[1000]), decode(np.squeeze(test_y[1000], -1)))"
      ],
      "outputs": [],
      "metadata": {
        "id": "6ylQu67xM0U6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build transformer model"
      ],
      "metadata": {
        "id": "ALWUKqUsPofT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\r\n",
        "  num_encoders = 4\r\n",
        "  num_docoders = 4\r\n",
        "  num_heads = 8\r\n",
        "  embed_size = 64 * num_docoders\r\n",
        "  drop_out_rate = 0.3\r\n",
        "  model = get_model(\r\n",
        "    token_num=len(rev_token_dict),\r\n",
        "    embed_dim=embed_size,\r\n",
        "    encoder_num=num_encoders,\r\n",
        "    decoder_num=num_docoders,\r\n",
        "    head_num=num_heads,\r\n",
        "    hidden_dim=embed_size,\r\n",
        "    attention_activation='gelu',\r\n",
        "    feed_forward_activation='gelu',\r\n",
        "    dropout_rate=drop_out_rate,\r\n",
        "    embed_weights=np.random.random((len(rev_token_dict), embed_size)),\r\n",
        "  )\r\n",
        "  model.compile(\r\n",
        "      optimizer=tf.keras.optimizers.Adam(),\r\n",
        "      loss='sparse_categorical_crossentropy',\r\n",
        "  )"
      ],
      "outputs": [],
      "metadata": {
        "id": "nVne6yNePqjc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "epochs = 60  # 60 is minimal to be meaningful\r\n",
        "batch_size = 128\r\n",
        "model.fit(\r\n",
        "  x=[train_x, train_x_d],\r\n",
        "  y=train_y,\r\n",
        "  batch_size=batch_size,\r\n",
        "  epochs=epochs,\r\n",
        "  validation_data=([test_x, test_x_d], test_y),\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "QE3KmHYUQX5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save your model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!mkdir -p {WORK_DIR}/model_weights\r\n",
        "model.save_weights(f'{WORK_DIR}/model_weights')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference, please see [this colab](https://github.com/hululuzhu/chinese-ai-writing-share/blob/main/RC_01_AI_Writing_Demo_06_2021.ipynb)"
      ],
      "metadata": {
        "id": "d6bU4cYq6e66"
      }
    }
  ]
}