{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "中文对联Transformer Source Code V1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOEMXK8iESZ51SnRyEIEPat",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/hululuzhu/chinese-ai-writing-share/blob/main/%E4%B8%AD%E6%96%87%E5%AF%B9%E8%81%94Transformer_Source_Code_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chinese Couplet Transformer model source code. e.g.\n",
        "\n",
        "```\n",
        "上: 欢天喜地度佳节\n",
        "下: 举国迎春贺新年\n",
        "上: 不待鸣钟已汗颜，重来试手竟何艰\n",
        "下: 只缘沧海常风雨，再去翻身只等闲\n",
        "上: 相思俱付三更月\n",
        "下: 寂寞难留一夜风\n",
        "```"
      ],
      "metadata": {
        "id": "VQAh-D0FCzwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to google drive and prepare local files"
      ],
      "metadata": {
        "id": "ntLVC-_5FaH3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive') # mount to google drive to save models after training\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "os.environ['TF_KERAS'] = '1'\r\n",
        "!pip install keras-transformer &> /dev/null\r\n",
        "from keras_transformer import get_model, decode, get_custom_objects\r\n",
        "\r\n",
        "import pathlib\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pickle"
      ],
      "outputs": [],
      "metadata": {
        "id": "R4tEmatSFNTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TPU setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
        "tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
        "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\r\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch Data and extract"
      ],
      "metadata": {
        "id": "i9kOwaYYfAAG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "working_dir = \"/tmp/working_dir\"\r\n",
        "!mkdir -p {working_dir}\r\n",
        "!wget https://github.com/wb14123/couplet-dataset/releases/download/1.0/couplet.tar.gz -P {working_dir}\r\n",
        "!ls -l {working_dir}"
      ],
      "outputs": [],
      "metadata": {
        "id": "gjU1zyfWFj4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!mkdir -p {working_dir}/couplet_files\r\n",
        "!tar -xf {working_dir}/couplet.tar.gz -C {working_dir}/couplet_files\r\n",
        "# !ls -l -R  /tmp/working_dir/couplet_files"
      ],
      "outputs": [],
      "metadata": {
        "id": "cSq9OXajj1hG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!head -1 {working_dir}/couplet_files/couplet/train/in.txt {working_dir}/couplet_files/couplet/train/out.txt"
      ],
      "outputs": [],
      "metadata": {
        "id": "ptcOb8M-Kq2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get vocabs of all chars"
      ],
      "metadata": {
        "id": "peOzerlpke4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "COUPLET_PATH = f'{working_dir}/couplet_files/couplet'\r\n",
        "token_dict = {\r\n",
        "    '<PAD>': 0,\r\n",
        "    '<START>': 1,\r\n",
        "    '<END>': 2,\r\n",
        "}\r\n",
        "with open(f\"{COUPLET_PATH}/vocabs\", \"r\") as f:\r\n",
        "  for x in f:\r\n",
        "    c = x.strip()[0]\r\n",
        "    if c not in token_dict:\r\n",
        "      token_dict[c] = len(token_dict)\r\n",
        "\r\n",
        "for t in ['train', 'test']:\r\n",
        "  for i in ['in', 'out']:\r\n",
        "    with open(f\"{COUPLET_PATH}/{t}/{i}.txt\", \"r\") as f:\r\n",
        "      for line in f:\r\n",
        "        for cs in line.strip().replace(' ', '').replace('\\n', ''):\r\n",
        "          for c in cs:\r\n",
        "            if c not in token_dict:\r\n",
        "              token_dict[c] = len(token_dict)\r\n",
        "\r\n",
        "assert 9132 == len(token_dict)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dvLAnInGj74m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with open(os.path.join('/content/gdrive/MyDrive/ML/Models/szhu_public_062021', 'couplet_vocab.pickle'), 'wb') as handle:\r\n",
        "    pickle.dump(token_dict, handle)"
      ],
      "outputs": [],
      "metadata": {
        "id": "YsfGuIet71lg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "rev_token_dict = {v: k for k, v in token_dict.items()}"
      ],
      "outputs": [],
      "metadata": {
        "id": "mDttIXl-l88h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode data (chars to char-ids)"
      ],
      "metadata": {
        "id": "BGHOCOZTLUDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "MAX_SEQ_LEN = 34  # 32 chars plus start/end\r\n",
        "\r\n",
        "def clean_input(rawq):\r\n",
        "  return rawq.strip().replace(' ', '')\r\n",
        "\r\n",
        "def encode(rawq, is_decode_output = False, is_2d=False):\r\n",
        "  output = []\r\n",
        "  if not is_decode_output:\r\n",
        "    output.append([1] if is_2d else 1) # start added to encode/decode outputs\r\n",
        "  # content encoding\r\n",
        "  string_leng = len(rawq.strip().replace(' ', ''))\r\n",
        "  for c in rawq.strip().replace(' ', ''):\r\n",
        "    if c not in token_dict:\r\n",
        "      token_dict[c] = len(token_dict)\r\n",
        "    output.append([token_dict[c]] if is_2d else token_dict[c])\r\n",
        "  output.append([2] if is_2d else 2) # end\r\n",
        "  for i in range(MAX_SEQ_LEN - len(output)):\r\n",
        "    output.append([0] if is_2d else 0) # padding to fixed MAX_SEQ_LEN size\r\n",
        "  return output\r\n",
        "\r\n",
        "train_raw = {\"in\": [], \"out\": [], \"pre\": [], \"post\": [], \"decode_in\": []}\r\n",
        "test_raw = {\"in\": [], \"out\": [], \"pre\": [], \"post\": [], \"decode_in\": []}\r\n",
        "total_raw = {'train': train_raw, 'test': test_raw}\r\n",
        "\r\n",
        "for t in ['train', 'test']:\r\n",
        "  for i in ['in', 'out']:\r\n",
        "    with open(f\"{COUPLET_PATH}/{t}/{i}.txt\", \"r\") as f:\r\n",
        "      for line in f:\r\n",
        "        if i == 'out':\r\n",
        "          total_raw[t]['decode_in'].append(encode(line, False, i=='in'))\r\n",
        "        total_raw[t][i].append(encode(line, i=='out', i=='out'))\r\n",
        "        total_raw[t][\"pre\" if i == 'in' else 'post'].append(clean_input(line))"
      ],
      "outputs": [],
      "metadata": {
        "id": "C7t-2Ly6LWTB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def decode_tokens(token_ids):\r\n",
        "  output = \"\"\r\n",
        "  for token_id in token_ids:\r\n",
        "    if token_id > 2:\r\n",
        "      output += rev_token_dict[token_id]\r\n",
        "    elif token_id == 0:\r\n",
        "      break\r\n",
        "  return output\r\n",
        "\r\n",
        "for inq, indecode, outq in zip(total_raw['train']['in'][:3],\r\n",
        "                     total_raw['train']['decode_in'][:3],\r\n",
        "                     total_raw['train']['out'][:3]):\r\n",
        "  print(inq, \"\\n\", indecode, \"\\n\", outq)\r\n",
        "  print(decode_tokens(inq), decode_tokens(np.asarray(outq).reshape(-1)))"
      ],
      "outputs": [],
      "metadata": {
        "id": "qEK47o_BOTkB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dfs = {}\r\n",
        "\r\n",
        "for t in ['train', 'test']:\r\n",
        "  dfs[t] = pd.DataFrame(\r\n",
        "      list(zip(total_raw[t]['in'], total_raw[t]['out'], total_raw[t]['pre'], total_raw[t]['post'], total_raw[t]['decode_in'])),\r\n",
        "      columns =['in', 'out', 'pre', 'post', 'decode_in'])\r\n",
        "  dfs[t]['in_length']  = dfs[t]['in'].str.len()\r\n",
        "  dfs[t]['out_length']  = dfs[t]['out'].str.len()\r\n",
        "  dfs[t]['de_in_length']  = dfs[t]['decode_in'].str.len()"
      ],
      "outputs": [],
      "metadata": {
        "id": "h8ynitPfQLsT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dfs['train'].describe()"
      ],
      "outputs": [],
      "metadata": {
        "id": "63PXWgKzSXKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model and training"
      ],
      "metadata": {
        "id": "keFfK9OWuNoM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "in_np = np.array(dfs['train']['in'].values.tolist())\r\n",
        "decode_in_np = np.array(dfs['train']['decode_in'].values.tolist())\r\n",
        "out_np = np.asarray(dfs['train']['out'].values.tolist())"
      ],
      "outputs": [],
      "metadata": {
        "id": "0465hKlj5vkE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "in_np_test = np.array(dfs['test']['in'].values.tolist())\r\n",
        "decode_in_np_test = np.array(dfs['test']['decode_in'].values.tolist())\r\n",
        "out_np_test = np.array(dfs['test']['out'].values.tolist())"
      ],
      "outputs": [],
      "metadata": {
        "id": "R6sMwRFtU-aA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(in_np.shape, decode_in_np.shape, out_np.shape)\r\n",
        "print(in_np_test.shape, decode_in_np_test.shape, out_np_test.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RYAaGohtXcSa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\r\n",
        "  num_encoders = 4\r\n",
        "  num_docoders = 4\r\n",
        "  num_heads = 8\r\n",
        "  embed_size = 64 * num_docoders\r\n",
        "  drop_out_rate = 0.1\r\n",
        "  model = get_model(\r\n",
        "    token_num=len(token_dict),\r\n",
        "    embed_dim=embed_size,\r\n",
        "    encoder_num=num_encoders,\r\n",
        "    decoder_num=num_docoders,\r\n",
        "    head_num=num_heads,\r\n",
        "    hidden_dim=embed_size,\r\n",
        "    attention_activation='gelu',\r\n",
        "    feed_forward_activation='gelu',\r\n",
        "    dropout_rate=drop_out_rate,\r\n",
        "    embed_weights=np.random.random((len(token_dict), embed_size)),\r\n",
        "  )\r\n",
        "  model.compile(\r\n",
        "      optimizer=tf.keras.optimizers.Adam(),\r\n",
        "      loss='sparse_categorical_crossentropy',\r\n",
        "  )"
      ],
      "outputs": [],
      "metadata": {
        "id": "0C4eYwYx_aYl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "epochs = 80\r\n",
        "batch_size = 256\r\n",
        "model.fit(\r\n",
        "  x=[in_np, decode_in_np],\r\n",
        "  y=out_np,\r\n",
        "  batch_size=batch_size,\r\n",
        "  epochs=epochs,\r\n",
        "  validation_data=([in_np_test, decode_in_np_test], out_np_test),\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ytBCHjck6yCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save model weights"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "DRIVE_MODEL_DIR = '/content/gdrive/MyDrive/ML/Models/chinese_couplet_v1'\r\n",
        "!mkdir -p {DRIVE_MODEL_DIR}\r\n",
        "model.save_weights(DRIVE_MODEL_DIR)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference, see [this colab](https://github.com/hululuzhu/chinese-ai-writing-share/blob/main/RC_01_AI_Writing_Demo_06_2021.ipynb)"
      ],
      "metadata": {
        "id": "qk0h37bTovKU"
      }
    }
  ]
}